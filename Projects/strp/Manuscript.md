# Manuscript

## Factor Analysis

Factor analysis assumes there exists a set of unobserved variables, or latent factors, $`z_j,j=1,2,...,k`$, that can explain the observed variables $`x_i,i=1,2,...,d`$ through linear transformation: $`x_i-μ_i=\sum_{j=1}^{k}v_{ij}z_j+ε_i`$, where $`μ_i`$ is the mean of the observed $`x_i`$, and $`v_{ij}`$ is the weights, or factor loading, of $`z_j`$ in terms of $`x_i`$, and $`ε_i`$ is the unobserved stochastic error terms of $`x_i`$ with zero mean.

Determining the optimal number of latent factors, i.e. $`k`$, is guided first by extended Bayesian information criteria (eBIC) followed by cross-validation score of machine learning models. In the terminology of model selection, both eBIC and cross-validation aim at increasing the generalizability of a model, or, to put it in another way, decreasing the overall discrepancy (OD), the difference between the population parameter and the estimated model. Choosing eBIC over other criteria is due to the strongly-controlled false discovery rate, in contrast to Akaike information criterion (AIC) or Bayes information criterion (BIC), which tends to select a model with spurious covariates. eBIC was applied to all possible values of k and only the top 10 candidates with the lowest eBIC score were considered for the next step. Cross validation was then performed to determine the optimal k by searching the value that yielded the highest accuracy of prediction on validation set.

## Learning Curve

Before moving on to training an SVC model, one may be interested in knowing whether the proposed model can even possibly learn from the limited number of samples available in the dataset. To argue that the sample size is large enough or not, learning curves can give us a hint. We randomly drew n samples from the original dataset and looped through every possible number of subsamples under the schema of four-fold cross validation. By plotting training and validation scores against the number of samples (as in Figure), we obtained the learning curves. If the learning curve of training set and that of validation set never meet or even go parallel, one can judge that the given number of samples is yet enough to train the model. Without doubt, the best solution is to recruit more participants to the study. Alternative solution may involve decreasing the complexity of the model.

## Linear Discriminant Analysis (LDA)

LDA is one of the most common model used in classification because of its ease of use and simplicity suitable for small dataset. LDA assumes that data points are normally distributed in $`d`$ dimensional space with respect to each class $`k`$, and that each class has the same covariance matrix $`Σ`$, i.e., data points in class $`k`$ follow a normal distribution $`N(μ_k,Σ)`$. The probability density function $`f_k`$ given class $`k`$ is $`f_k(x)=\frac{1}{(2\pi)^{d/2}|Σ_k|^{1/2}}\exp(-\frac{1}{2}(x-μ_k)^{\top}Σ_k^{-1}(x-μ_k))`$, where $`Σ_k`$ is the covariance matrix of data points in class $`k`$, and $`Σ=Σ_1=…=Σ_k`$ in the case of LDA. In the binary case, the class to which an observation belongs is determined by whether the log likelihood ratio is above or below a threshold $`T`$, i.e. the class is $`\begin{cases}1\ \text{if}\ \log\frac{P(Y=1|X=x)}{P(Y=0|X=x)}=-\frac{1}{2}[(x-μ_1)^{\top}Σ_1^{-1}(x-μ_1)-(x-μ_0)^{\top}Σ_0^{-1}(x-μ_0)]>T\\0\ \text{otherwise}\end{cases}`$. The default value of $`T`$ is zero in a sense that an observation simply belongs to the class yielding greater probability. The resulting decision boundary of a linear discriminant classifier is a hyperplane in $`d`$ dimensions. Given a dataset of $`k`$ classes in $`d`$ dimensional space, the number of parameters to estimate is $`kd+d^2`$.

However, pure LDA may suffer from underfitting and overfitting when the dataset has much more observations than features or much more features than observations. To avoid this shortage, regularized (penalized) LDA was developed to overcome overfitting through the “shrinkage” of covariance matrix to regularize (penalize) large number of highly correlated features. The resulting covariance matrix after shrinkage ($`α`$) is $`Σ_k(\alpha)=(1-\alpha)Σ_k+αI`$, where $`α\in[0,1]`$. LDA used in order for classification is termed as linear discriminant classifier.

## Support Vector Classifier with Gaussian Kernel

Support vector machine (SVM) has its origin in 1930s and evolved to the current form in 1990s. SVM bases its original theory on linear form, but nonlinearity is achieved through the introduction of kernel functions, which transform the original features to higher orders in an efficient manner.

The classification version of SVM in conjunction with Gaussian (or radial basis function) kernel, or Gaussian kernel support vector classifier (SVC), is our focus in this study in hopes of achieving a satisfying accuracy in predicting both chronic and first-episode patients to be remitted or non-remitted.

SVC is essentially a way of searching a hyperplane that best separate data points of different classes through optimization (minimization of a loss function). The loss function, in the case of SVC, is the sum of errors incurred by misclassifications. In terms of $`w`$ (the normal vector of the separating hyperplane), the loss function is defined as $`J(w;x,y)=Σ_{i=1}^n\max\{0,1-y_iw^{\top}x_i\}`$

In addition to finding the separating hyperplane, SVC also adjusts the hyperplane to achieve the maximum margin through a technique commonly referred to as regularization. In our case of an L2 regularization, i.e. $`\|w\|_2`$, minimizing $`\|w\|_2`$ is essentially equivalent to maximizing the margin. The new loss function then becomes the sum of two expressions, an error term and an regularization term: $`J(w,C;x,y)=CΣ_{i=1}^n\max\{0,1-y_iw^{\top}x_i\}+\|w\|_2`$, where $`C`$ determines the penalty on the error term. The higher $`C`$ is, the more a model overfits the data, since misclassification is more intolerable and remarkably penalized during the process of optimizing the loss function.

Last, to learn from data not linearly separable, we relied on Gaussian kernel to transform the original features to higher orders efficiently. Given any two features $`x_i`$ and $`x_j`$ in the original dataset, the Gaussian kernel takes the form: $`K(x_i,x_j)=\exp(γ\|x_i-x_j\|^2)`$

Putting all these together, we build a Gaussian kernel SVC with the following steps: (1) Transform the original dataset with Gaussian kernel. (2) Define an loss function, the objective of optimization. (3) Perform optimization and learn the parameter of our interest. After the learning pipeline, an SVC will tell us the optimal $`w`$, i.e. the normal vector of the separating hyperplane. And the class of any input test query is determined by whether the data point is above or below the hyperplane.

## Uncinate Fasciculus (UF)

The uncinate fasciculus (UF) is part of the fronto-temporal association systems, connecting the frontal lobe and the limbic system in the temporal lobe. The role of the UF has been strongly suggested in chronic schizophrenia. Bilaterally reduced FA in the UF have been reproduced by several prior studies (Burns et al., 2003; Huang et al., 2018; Kubicki et al., 2002; Mori et al., 2007). In our study, bilateral UF had a reduction in both FA and GFA, and an increase in RD. Increase in MD was also found in the left UF. Altogether, these findings suggest that poor outcomes are related to an unclear disease process happening in the UF, causing decreased axonal density and less diffusion barrier along the tract.

## Cingulum Bundle

Cingulate white matter anatomically connects cingulate gyrus and entorhinal cortex located in the medial temporal lobe. Cingulate white matter has long been thought to be responsible for interaction between cognition and memory, and its damage leads to mild cognitive impairment (MCI). Subnormal levels of FA in two cingulum sub-connections was found to be correlated with the severity of patients' positive symptoms, specifically, delusions and hallucinations (Whitford et al., 2014). It would not be surprising that we reproduced the same findings, that there was a decrease in FA in the cingulum bundle, implying tarnished white matter tract integrity is most likely due to decreased axonal density in that area. Loss of white matter tract integrity in the cingulate white matter not only heralds the severity of positive symptoms but also the poor response to antipsychotics.

##  Superior Longitudinal Fasciculus I (SLF-I)

The SLF-I, which connects the superior frontal gyrus and the precuneus in the medial parietal cortex, is believed to coordinate motor behavior and decision making. Decreased FA in the SLF has been identified in schizophrenia by several studies (Burns et al., 2003; Hubl et al., 2004; Kubicki et al., 2005; Federspiel et al., 2006; Mendelsohn et al., 2006; Buchsbaum et al., 2006).
