# Draft

## Factor analysis
- Factor analysis assumes there exists a set of unobserved variables, or latent factors, $z_j, j=1,2,...,k$, that can explain the observed variables $x_i, i=1,2,...,d$ through linear transformation: $x_i-\mu_i = \sum_{j=1}^k v_{ij}z_j + \epsilon_i$, where $\mu_i$ is the mean of the observed $x_i$, and $v_{ij}$ is the weights, or factor loading, of $z_j$ in terms of $x_i$, and $\epsilon_i$ is the unobserved stochastic error terms of $x_i$ with zero mean.
- Determining the optimal number of latent factors, i.e. k, is guided first by extended Bayesian information criteria (eBIC) followed by cross-validation score of machine learning models. In the terminology of model selection, both eBIC and cross-validation aim at increasing the generalizability of a model, or, to put it in another way, decreasing the overall discrepancy (OD), the difference between the population parameter and the estimated model. Choosing eBIC over other criteria is due to the strongly-controlled false discovery rate, in contrast to Akaike information criterion (AIC) or Bayes information criterion (BIC), which tends to select a model with spurious covariates. eBIC was applied to all possible values of k and only the top 10 candidates with the lowest eBIC score were considered for the next step. Cross validation was then performed to determine the optimal k by searching the value that yielded the highest accuracy of prediction on validation set.

## Learning curves
- Before moving on to training an SVC model, one may be interested in knowing whether the proposed model can even possibly learn from the limited number of samples available in the dataset. To argue that the sample size is large enough or not, learning curves can give us a hint. We randomly drew n samples from the original dataset and looped through every possible number of subsamples under the schema of four-fold cross validation. By plotting training and validation scores against the number of samples (as in Figure), we obtained the learning curves. If the learning curve of training set and that of validation set never meet or even go parallel, one can judge that the given number of samples is yet enough to train the model. Without doubt, the best solution is to recruit more participants to the study. Alternative solution may involve decreasing the complexity of the model.

## Logistic regression (LR)
- LR achieves classification from completely different approaches compared to LDA and QDA. LR aims at finding a hyperplane that best separates data points of different classes through optimizing (minimizing) a loss function designed to penalize more on each misclassification. Letâ€™s consider classifying a dataset $X =[x_1,x_2,...,x_n]$ of $n$ observations with exactly two classes $y = [y_1,y_2,...,y_n], y\in\{-1,1\}$. The loss function usually takes on the form $J(w;x,y) = \Sigma_{i=1}^n \log(1+\exp(-y_iw^\text{T}x_i))$, where the target variables of optimization is $w$, the normal vector of the hyperplane.
- In many situations, we would add a regularization term to prevent $w$ from ending up with extreme values, i.e. to make $w$ as close to zero as possible. In a sense, regularization also avoids overfitting. L2 regularization, i.e. $\|w\|_2$, was adopted in our study. The resulting objective function of optimization becomes $J(w,C;x,y) = C\Sigma_{i=1}^n \log(1+\exp(-y_iw^\text{T}x_i)) + \|w\|_2$, where $C$ determines the strength of regularization, with smaller values implying stronger regularization.

## Support vector classifier with Gaussian kernel
- Support vector machine (SVM) has its origin in 1930s and evolved to the current form in 1990s. SVM bases its original theory on linear form, but nonlinearity is achieved through the introduction of kernel functions, which transform the original features to higher orders in an efficient manner.
- The classification version of SVM in conjunction with Gaussian (or radial basis function) kernel, or Gaussian kernel support vector classifier (SVC), is our focus in this study in hopes of achieving a satisfying accuracy in predicting both chronic and first-episode patients to be remitted or non-remitted.
- SVC is essentially a way of searching a hyperplane that best separate data points of different classes through optimization (minimization of a loss function). The loss function, in the case of SVC, is the sum of errors incurred by misclassifications. In terms of $w$ (the normal vector of the separating hyperplane), the loss function is defined as: $J(w; x, y) = \Sigma_{i=1}^n \max\{0, 1 - y_i w^\top x_i\}$
- In addition to finding the separating hyperplane, SVC also adjusts the hyperplane to achieve the maximum margin through a technique commonly referred to as regularization. In our case of an L2 regularization, i.e. $\|w\|_2$, minimizing $\|w\|_2$ is essentially equivalent to maximizing the margin. The new loss function then becomes the sum of two expressions, an error term and an regularization term: $J(w, C; x, y) = C \Sigma_{i=1}^n \max\{0, 1 - y_i w^\top x_i\} + \|w\|_2$, where $C$ determines the penalty on the error term. The higher $C$ is, the more a model overfits the data, since misclassification is more intolerable and remarkably penalized during the process of optimizing the loss function.
- Last, to learn from data not linearly separable, we relied on Gaussian kernel to transform the original features to higher orders efficiently. Given any two features xi and xj in the original dataset, the Gaussian kernel takes the form: $K(x_i, x_j) = \exp(\gamma\|x_i-x_j\|^2)$
- Putting all these together, we build a Gaussian kernel SVC with the following steps: (1) Transform the original dataset with Gaussian kernel. (2) Define an loss function, the objective of optimization. (3) Perform optimization and learn the parameter of our interest. After the learning pipeline, an SVC will tell us the optimal $w$, i.e. the normal vector of the separating hyperplane. And the class of any input test query is determined by whether the data point is above or below the hyperplane.

## Uncinate fasciculus (UF)

- The uncinate fasciculus (UF) is part of the fronto-temporal association systems, connecting the frontal lobe and the limbic system in the temporal lobe. The role of the UF has been strongly suggested in chronic schizophrenia. Bilaterally reduced FA in the UF have been reproduced by several prior studies (Burns et al., 2003; Huang et al., 2018; Kubicki et al., 2002; Mori et al., 2007). In our study, bilateral UF had a reduction in both FA and GFA, and an increase in RD. Increase in MD was also found in the left UF. Altogether, these findings suggest that poor outcomes are related to an unclear disease process happening in the UF, causing decreased axonal density and less diffusion barrier along the tract.

## Cingulum bundle

- Cingulate white matter anatomically connects cingulate gyrus and entorhinal cortex located in the medial temporal lobe. Cingulate white matter has long been thought to be responsible for interaction between cognition and memory, and its damage leads to mild cognitive impairment (MCI). Subnormal levels of FA in two cingulum sub-connections was found to be correlated with the severity of patients' positive symptoms, specifically, delusions and hallucinations (Whitford et al., 2014). It would not be surprising that we reproduced the same findings, that there was a decrease in FA in the cingulum bundle, implying tarnished white matter tract integrity is most likely due to decreased axonal density in that area. Loss of white matter tract integrity in the cingulate white matter not only heralds the severity of positive symptoms but also the poor response to antipsychotics.

##  Superior longitudinal fasciculus I (SLF-I)

- The SLF-I, which connects the superior frontal gyrus and the precuneus in the medial parietal cortex, is believed to coordinate motor behavior and decision making. Decreased FA in the SLF has been identified in schizophrenia by several studies (Burns et al., 2003; Hubl et al., 2004; Kubicki et al., 2005; Federspiel et al., 2006; Mendelsohn et al., 2006; Buchsbaum et al., 2006).
